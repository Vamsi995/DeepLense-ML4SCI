{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK9G2Fm_92ds"
      },
      "source": [
        "## Initializing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLFcsS0QaoiA"
      },
      "outputs": [],
      "source": [
        "# Mounting drive and importing dataset from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw9zeWbyfohv"
      },
      "outputs": [],
      "source": [
        "!unzip /content/gdrive/MyDrive/Copy\\ of\\ dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OnsdUahThjLV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q43DtHD_jfsg"
      },
      "outputs": [],
      "source": [
        "numpy_no = []\n",
        "numpy_sphere = []\n",
        "numpy_vort = []\n",
        "numpy_no_val = []\n",
        "numpy_sphere_val = []\n",
        "numpy_vort_val = []\n",
        "\n",
        "for np_name in glob.glob('/content/dataset/train/no/*.np[yz]'):\n",
        "    numpy_no.append(np.squeeze(np.load(np_name)))\n",
        "\n",
        "\n",
        "for np_name in glob.glob('/content/dataset/train/sphere/*.np[yz]'):\n",
        "    numpy_sphere.append(np.squeeze(np.load(np_name)))\n",
        "\n",
        "\n",
        "for np_name in glob.glob('/content/dataset/train/vort/*.np[yz]'):\n",
        "    numpy_vort.append(np.squeeze(np.load(np_name)))\n",
        "\n",
        "\n",
        "for np_name in glob.glob('/content/dataset/val/no/*.np[yz]'):\n",
        "    numpy_vort.append(np.squeeze(np.load(np_name)))\n",
        "\n",
        "\n",
        "for np_name in glob.glob('/content/dataset/val/sphere/*.np[yz]'):\n",
        "    numpy_vort.append(np.squeeze(np.load(np_name)))\n",
        "\n",
        "\n",
        "for np_name in glob.glob('/content/dataset/val/vort/*.np[yz]'):\n",
        "    numpy_vort.append(np.squeeze(np.load(np_name)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "63QdqgIXpAK2"
      },
      "outputs": [],
      "source": [
        "nox_train, nox_test, noy_train, noy_test = train_test_split(numpy_no, np.zeros(len(numpy_no)), test_size=0.1)\n",
        "spherex_train, spherex_test, spherey_train, spherey_test = train_test_split(numpy_sphere, np.ones(len(numpy_sphere)), test_size=0.1)\n",
        "vortx_train, vortx_test, vorty_train, vorty_test = train_test_split(numpy_vort, np.full(len(numpy_vort), 2), test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EKNqtb93uFLe"
      },
      "outputs": [],
      "source": [
        "X_train = nox_train + spherex_train + vortx_train\n",
        "X_test = nox_test + spherex_test + vortx_test\n",
        "Y_train = np.concatenate((noy_train, spherey_train, vorty_train), axis=None)\n",
        "Y_test = np.concatenate((noy_test, spherey_test, vorty_test), axis=None)\n",
        "X_val = numpy_no_val + numpy_sphere_val + numpy_vort_val\n",
        "Y_val = np.concatenate((np.zeros(len(numpy_no_val)), np.ones(len(numpy_sphere_val)), np.full(len(numpy_vort_val), 2)), axis=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapIndexPosition = list(zip(X_train, Y_train))\n",
        "random.shuffle(mapIndexPosition)\n",
        "# make list separate\n",
        "X_train, Y_train = zip(*mapIndexPosition)"
      ],
      "metadata": {
        "id": "pp6HlBZWBwch"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = list(X_train)\n",
        "Y_train = np.array(list(Y_train))"
      ],
      "metadata": {
        "id": "SNyUgZjxCaRd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKKL7wAG97tA"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U0x8firykrlA"
      },
      "outputs": [],
      "source": [
        "class ToNumpy(object):\n",
        "    def __call__(self, sample):\n",
        "        return np.array(sample)\n",
        "\n",
        "def fix_compose_transform(transform):\n",
        "        if isinstance(transform.transforms[-1], torchvision.transforms.ToTensor):\n",
        "            transform = torchvision.transforms.Compose([\n",
        "                *transform.transforms[:-1],\n",
        "                ToNumpy(),\n",
        "                torchvision.transforms.ToTensor()\n",
        "            ])\n",
        "        return transform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ojPnkMP7tbBs"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X_train, Y_train, X_test, Y_test, X_val, Y_val, phase):\n",
        "        self.phase = phase\n",
        "        self.X_train = X_train\n",
        "        self.Y_train = Y_train\n",
        "        self.X_test = X_test\n",
        "        self.Y_test = torch.from_numpy(Y_test).type(torch.LongTensor)\n",
        "        self.tensor_Y_train = torch.from_numpy(Y_train).type(torch.LongTensor)\n",
        "        self.X_val = X_val\n",
        "        self.Y_val = torch.from_numpy(Y_val).type(torch.LongTensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.phase == 'train':\n",
        "\n",
        "            trainsample = self.X_train[idx]\n",
        "            im = Image.fromarray(trainsample)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                                            # transforms.RandomRotation(45),\n",
        "                                            # transforms.RandomHorizontalFlip(),\n",
        "                                            # transforms.RandomVerticalFlip(),\n",
        "                                            # transforms.Resize((150, 150)),\n",
        "                                            transforms.ToTensor()\n",
        "                                            ])\n",
        "            \n",
        "            fix_transform = fix_compose_transform(transform)\n",
        "            data = fix_transform(im)\n",
        "            label = self.tensor_Y_train[idx]\n",
        "            return data, label\n",
        "\n",
        "        elif self.phase == 'test':\n",
        "\n",
        "            testsample = self.X_test[idx]\n",
        "            im = Image.fromarray(testsample)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                                            # transforms.Resize((150, 150)),\n",
        "                                            transforms.ToTensor()\n",
        "                                            ])\n",
        "            \n",
        "            fix_transform = fix_compose_transform(transform)\n",
        "            data = fix_transform(im)\n",
        "            label = self.Y_test[idx]\n",
        "            return data, label\n",
        "\n",
        "        elif self.phase == 'val':\n",
        "            testsample = self.X_test[idx]\n",
        "            im = Image.fromarray(testsample)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                                            # transforms.Resize((150, 150)),\n",
        "                                            transforms.ToTensor()\n",
        "                                            ])\n",
        "            \n",
        "            fix_transform = fix_compose_transform(transform)\n",
        "            data = fix_transform(im)\n",
        "            label = self.Y_test[idx]\n",
        "            return data, label\n",
        "            \n",
        "            \n",
        "    def __len__(self):\n",
        "        if self.phase == \"train\":\n",
        "          return len(X_train)\n",
        "        else:\n",
        "          return len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qynhFfgsto6x"
      },
      "outputs": [],
      "source": [
        "training_data = CustomDataset(X_train, Y_train, X_test, Y_test, X_val, Y_val, 'train')\n",
        "testing_data = CustomDataset(X_train, Y_train, X_test, Y_test, X_val, Y_val, 'test')\n",
        "validation_data = CustomDataset(X_train, Y_train, X_test, Y_test, X_val, Y_val, 'val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "sHkcEbHQu-7y"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(training_data, batch_size=100, shuffle=True)\n",
        "test_loader = DataLoader(testing_data, batch_size=100, shuffle=False)\n",
        "val_loader = DataLoader(validation_data, batch_size=100, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del numpy_no\n",
        "del numpy_sphere\n",
        "del numpy_vort\n",
        "del numpy_no_val\n",
        "del numpy_sphere_val\n",
        "del numpy_vort_val"
      ],
      "metadata": {
        "id": "FCyu9E7TtN2S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK8pbwSx9Jjg"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xwU7APONvVjl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                    stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(1, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(8)\n",
        "        self.fc1 = nn.Linear(8*8*64, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 500)\n",
        "        self.fc3 = nn.Linear(500, 250)\n",
        "        self.fc4 = nn.Linear(250, 100)\n",
        "        self.fc5 = nn.Linear(100, 3)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(-1, 8*8*64)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = F.relu(self.fc3(out))\n",
        "        out = F.relu(self.fc4(out))\n",
        "        out = self.fc5(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9mYw_k9t3iK",
        "outputId": "50ede186-3218-4283-e407-9beec4d6f040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "batch_size = 10\n",
        "net_args = {\n",
        "    \"block\": ResidualBlock,\n",
        "    \"layers\": [2, 2, 2, 2]\n",
        "}\n",
        "model = ResNet(**net_args)\n",
        "\n",
        "model.to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Hap948qIFKQV"
      },
      "outputs": [],
      "source": [
        "def testAccuracy():\n",
        "    \n",
        "    model.eval()\n",
        "    accuracy = 0.0\n",
        "    total = 0.0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # run the model on the test set to predict labels\n",
        "            outputs = model(images)\n",
        "            # the label with the highest energy will be our prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            accuracy += (predicted == labels).sum().item()\n",
        "    \n",
        "    # compute the accuracy over all test images\n",
        "    accuracy = (100 * accuracy / total)\n",
        "    return(accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ialPyDJAyGdm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "min_valid_loss = np.inf\n",
        " \n",
        "for e in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    for data, labels in train_loader:\n",
        "        # Transfer Data to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "         \n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "        # Forward Pass\n",
        "        target = model(data)\n",
        "        # Find the Loss\n",
        "        loss = criterion(target,labels)\n",
        "        # Calculate gradients\n",
        "        loss.backward()\n",
        "        # Update Weights\n",
        "        optimizer.step()\n",
        "        # Calculate Loss\n",
        "        train_loss += loss.item()\n",
        "     \n",
        "    valid_loss = 0.0\n",
        "    model.eval()     # Optional when not using Model Specific layer\n",
        "    for data, labels in val_loader:\n",
        "        # Transfer Data to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            data, labels = data.cuda(), labels.cuda()\n",
        "         \n",
        "        # Forward Pass\n",
        "        target = model(data)\n",
        "        # Find the Loss\n",
        "        loss = criterion(target,labels)\n",
        "        # Calculate Loss\n",
        "        valid_loss += loss.item()\n",
        " \n",
        "    print(f'Epoch {e+1} \\t\\t Training Loss: {train_loss / len(train_loader)} \\t\\t Validation Loss: {valid_loss / len(val_loader)} \\t\\t Accuracy: {testAccuracy()}')\n",
        "     \n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "         \n",
        "        # Saving State Dict\n",
        "        torch.save(model.state_dict(), 'saved_model.pth')"
      ],
      "metadata": {
        "id": "5AkaJjhlptJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd2MXpbd-AZg"
      },
      "source": [
        "## Loss and Accuracy Plots - ROC Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "zdeQhqtu2C3X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(testing_data, batch_size=1, shuffle=False)"
      ],
      "metadata": {
        "id": "2dGmTvlK1Wj2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "accuracy = 0.0\n",
        "total = 0.0\n",
        "\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # run the model on the test set to predict labels\n",
        "        outputs = model(images)\n",
        "        # the label with the highest energy will be our prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        accuracy += (predicted == labels).sum().item()\n",
        "        y_pred.append(predicted.cpu().numpy()[0])\n",
        "\n",
        "# compute the accuracy over all test images\n",
        "accuracy = (100 * accuracy / total)\n",
        "\n"
      ],
      "metadata": {
        "id": "aeuJRQaMvxQd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics as metrics\n",
        "# calculate the fpr and tpr for all thresholds of the classification\n",
        "fpr, tpr, threshold = metrics.roc_curve(Y_test, y_pred, pos_label=1)\n",
        "roc_auc = metrics.auc(fpr, tpr)\n",
        "\n",
        "# method I: plt\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_KvXGlNB2-Wg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Dark Matter Substructure Multi-Class Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}